# Multi-XGB
import pandas as pd
import numpy as np
from numpy import *
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from sklearn.metrics import log_loss, roc_auc_score,f1_score
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score
import keras
import math
from sklearn.model_selection import KFold
from keras.wrappers.scikit_learn import KerasClassifier
from xgboost.sklearn import XGBClassifier
import xgboost as xgb
from sklearn.utils.class_weight import compute_sample_weight
from keras.utils import to_categorical
import os
import warnings
warnings.filterwarnings("ignore")

### load data
df = pd.read_csv('/home/kesci/input/data7617/dat.1944.csv')
tdf = pd.read_csv('/home/kesci/input/data7617/dat.60.csv')
df = df.replace('ASD', 0)
df = df.replace('DLD', 1)
df = df.replace('GDD', 2)
tdf = tdf.replace('ASD', 0)
tdf = tdf.replace('DLD', 1)
tdf = tdf.replace('GDD', 2)
col = df.columns[1:]
col1 = df.columns[2:]
dat = df[col]
grp = dat['diag']
x_test = tdf[col1]
y_test = tdf['diag']

x_train = dat[col[1:]]
y_train = dat['diag']

xlf = XGBClassifier(
max_depth = 4,
learning_rate = 0.0001,
n_estimators = 4000,
silent = True,
eval_metric = 'merror',
objective = 'multi:softprob',
gamma = 0.2,
subsample = 0.7,
colsample_bytree = 0.8,
num_class = 3)  

xgc = xlf

xgc.fit(x_train, y_train, sample_weight = compute_sample_weight("balanced", y_train))
pred_test = xgc.predict_proba(x_test)
  ### predict test
pred_res = np.argmax(pred_test, axis=1)
mat_test = confusion_matrix(y_test, pred_res)
acc_all_test = np.diag(mat_test)/mat_test.sum(axis=1)
acc_test = accuracy_score(y_test, pred_res)
acc1 = acc_all_test[0]
acc2 = acc_all_test[1]
acc3 = acc_all_test[2]
print('ACC:\n', acc_test)
print('R:\n', acc_all_test)
print('Matrix:\n', mat_test)
y = pd.DataFrame(pred_res)
y.to_csv('/home/kesci/work/y60.csv', index=False)

   

### Stage one 
import pandas as pd
import numpy as np
from numpy import *
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from sklearn.metrics import log_loss, roc_auc_score,f1_score
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score
import keras
import math
from sklearn.model_selection import KFold
from keras.wrappers.scikit_learn import KerasClassifier
from xgboost.sklearn import XGBClassifier
import xgboost as xgb
from sklearn.utils.class_weight import compute_sample_weight
from keras.utils import to_categorical
import os
import warnings
warnings.filterwarnings("ignore")

### load data
df = pd.read_csv('/home/kesci/input/data7617/dat.1944.csv')
tdf = pd.read_csv('/home/kesci/input/data7617/dat.60.csv')
df = df.replace('ASD', 0)
df = df.replace('DLD', 1)
df = df.replace('GDD', 0)
tdf = tdf.replace('ASD', 0)
tdf = tdf.replace('DLD', 1)
tdf = tdf.replace('GDD', 0)
col = col = ['diag','adaptation','social','fine','gross']
col1 = ['adaptation','social','fine','gross']
dat = df[col]
grp = dat['diag']
x_test = tdf[col1]
y_test = tdf['diag']

x_train = dat[col[1:]]
y_train = dat['diag']

xlf = XGBClassifier(
max_depth = 3,
learning_rate = 0.0001,
n_estimators = 1,
silent = True,
eval_metric = 'error',
 objective = 'binary:logistic',
gamma = 0.2,
subsample = 1,
colsample_bytree = 1)  

xgc = xlf

xgc.fit(x_train, y_train, sample_weight = compute_sample_weight("balanced", y_train))
pred_test = xgc.predict_proba(x_test)
  ### predict test
pred_res = np.argmax(pred_test, axis=1)
mat_test = confusion_matrix(y_test, pred_res)
acc_all_test = np.diag(mat_test)/mat_test.sum(axis=1)
acc_test = accuracy_score(y_test, pred_res)
acc1 = acc_all_test[0]
acc2 = acc_all_test[1]

print('ACC:\n', acc_test)
print('R:\n', acc_all_test)
print('Matrix:\n', mat_test)

xgb.to_graphviz(xgc, num_trees=0)

### Stage two is similar to Stage one

